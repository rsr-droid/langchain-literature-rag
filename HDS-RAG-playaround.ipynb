{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv openai-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai-env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = 'C:\\\\Users\\\\Rajan\\\\OneDrive - University College London\\\\MSc - Multimodal Project\\\\Literature\\\\Papers\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader,PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniXGen: A UniÔ¨Åed Vision-Language Model for\n",
      "Multi-View Chest X-ray Generation and Report\n",
      "Generation\n",
      "Hyungyung Lee1, Da Young Lee2‚àó, Wonjae Kim3, Jin-Hwa Kim3,4\n",
      "Tackeun Kim5, Jihang Kim5, Leonard Sunwoo5, Edward Choi1\n",
      "1KAIST,2Deep-in-Sight Co.,3Naver AI Lab,4SNU AIIS,\n",
      "5Seoul National University Bundang Hospital\n",
      "ttumyche@kaist.ac.kr, dyan.lee717@gmail.com, wonjae.kim@navercorp.com ,\n",
      "j1nhwa.kim@navercorp.com, tackeun.kim@snu.ac.kr, radio622@gmail.com ,\n",
      "leonard.sunwoo@gmail.com, edwardchoi@kaist.ac.kr\n",
      "Abstract\n",
      "Generated synthetic data in medical research can substitute privacy and security-\n",
      "sensitive data with a large-scale curated dataset, reducing data collection and\n",
      "annotation costs. As part of this effort, we propose UniXGen, a uniÔ¨Åed chest X-ray\n",
      "and report generation model, with the following contributions. First, we design a\n",
      "uniÔ¨Åed model for bidirectional chest X-ray and report generation by adopting a\n",
      "vector quantization method to discretize chest X-rays into discrete visual tokens and\n",
      "formulating both tasks as sequence generation tasks. Second, we introduce several\n",
      "special tokens to generate chest X-rays with speciÔ¨Åc views that can be useful when\n",
      "the desired views are unavailable. Furthermore, UniXGen can Ô¨Çexibly take various\n",
      "inputs from single to multiple views to take advantage of the additional Ô¨Åndings\n",
      "available in other X-ray views. We adopt an efÔ¨Åcient transformer for computational\n",
      "and memory efÔ¨Åciency to handle the long-range input sequence of multi-view chest\n",
      "X-rays with high resolution and long paragraph reports. In extensive experiments,\n",
      "we show that our uniÔ¨Åed model has a synergistic effect on both generation tasks, as\n",
      "opposed to training only the task-speciÔ¨Åc models. We also Ô¨Ånd that view-speciÔ¨Åc\n",
      "special tokens can distinguish between different views and properly generate\n",
      "speciÔ¨Åc views even if they do not exist in the dataset, and utilizing multi-view chest\n",
      "X-rays can faithfully capture the abnormal Ô¨Åndings in the additional X-rays. The\n",
      "source code is publicly available at: https://github.com/ttumyche/UniXGen.\n",
      "1 Introduction\n",
      "Patient privacy, imbalanced class distribution, the need for trained clinicians, and the lack of large\n",
      "publicly available datasets are chronic problems in medical research. In an effort to alleviate these\n",
      "problems, research on synthetic data generation has been actively explored. Among them, the\n",
      "combination of chest radiographs and radiology reports has made signiÔ¨Åcant progress, as it provides a\n",
      "comprehensive examination of the patient‚Äôs condition that helps diagnose and detect various diseases.\n",
      "In response, we propose a uniÔ¨Åed chest X-ray and radiology report generation model with multiple\n",
      "views, namely UniXGen, with the following improvements.\n",
      "First, we design a uniÔ¨Åed model for bidirectional chest X-ray and report generation. Although these\n",
      "tasks are bidirectional, existing works propose task-speciÔ¨Åc architectures and develop them separately.\n",
      "We simplify the design effort by utilizing VQ-GAN [ 7] as an image tokenizer. This approach allows\n",
      "‚àóWork done at KAIST\n",
      "Preprint.arXiv:2302.12172v4  [eess.IV]  11 Apr 2023\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of tokens that gpt-3.5-turbo will use for some text we need to initialize the tiktoken tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the tokenizer we defined the encoder as \"cl100k_base\". This is a specific tiktoken encoder which is used by gpt-3.5-turbo, as well as gpt-4, and text-embedding-ada-002 which are models supported by OpenAI at the time of this writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the __tiktoken_len__ function, let's count and visualize the number of tokens across our webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = [tiktoken_len(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see min, average, and max values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 27\n",
      "Avg: 1129\n",
      "Max: 32261\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set style and color palette for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(token_counts, kde=False, bins=50)\n",
    "\n",
    "# customize the plot info\n",
    "plt.title(\"Token Counts Histogram\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Text\n",
    "At the time of writing, gpt-3.5-turbo supports a context window of 4096 tokens ‚Äî that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
    "\n",
    "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into gpt-3.5-turbo, leaving ~2000 tokens for conversation history and completion.\n",
    "\n",
    "With this ~2000 token limit we may want to include five snippets of relevant information, meaning each snippet can be no more than 400 token long.\n",
    "\n",
    "To create these snippets we use the RecursiveCharacterTextSplitter from LangChain. To measure the length of snippets we also need a length function. This is a function that consumes text, counts the number of tokens within the text (after tokenization using the gpt-3.5-turbo tokenizer), and returns that number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n','\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the text for a document like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(docs[5].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 374, 112)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1.\\n4.2.3 Human Evaluation\\nUsing 100 triples of an original chest X-ray generated chest X-rays from our model and a baseline,\\nwe ask three board-certiÔ¨Åed clinicians to evaluate each chest X-ray on three aspects: (1) realism, (2)\\nalignment with the given report, and (3) the view position among PA, AP, and LATERAL views. Both\\n(1) and (2) are rated on a scale from 1 (worst) to 5 (best). The triples consist of 33 triples from PA and\\nAP and 34 triples from LATERAL. The clinicians consist of two radiologists and one neurosurgeon,\\nand the X-rays are presented in random order for each triple.\\n4.3 Experiment Design\\nExperiments are designed to investigate the followings:\\nThe Advantage of the UniÔ¨Åed Model\\nWe evaluate the advantage of a bidirectional uniÔ¨Åed model compared to separate models for chest\\nX-ray and radiology report generation. There are four variants: 1) Single AP, 2) Single PA, 3)\\nSingle LAT. , and 4) Single report . Each model is only trained to maximize the log-likelihood of the\\ntokens corresponding to its speciÔ¨Åc modality.\\nThe Effect of Multi-view Chest X-rays\\nTo evaluate the effect of using multi-view chest X-rays on the generation quality, we divide the test\\ndataset into three groups based on the number of chest X-rays per study. These groups include studies\\nwith one X-ray (S w/1), two X-rays (S w/2), and three X-rays (S w/3). We evaluate our model by\\nincrementally increasing the number of input chest X-rays within each group. For example, in the'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For docs[5] we created 2 chunks of token length 391, 374 and 112."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Knowledge Base - Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create a FAISS index for the specified dimension\n",
    "dimension = 1536\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
    "\n",
    "# Embedding model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 813/813 [11:24<00:00,  1.19it/s]  \n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "embeddings = [] \n",
    "\n",
    "# Loop over the documents\n",
    "for doc in tqdm.tqdm(docs):  # Added tqdm for progress tracking\n",
    "    # Extract the text content from each Document object\n",
    "    doc_text = doc.page_content\n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(doc_text)\n",
    "    # Create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\"chunk\": j, \"text\": doc_text} for j, doc_text in enumerate(chunks)]\n",
    "\n",
    "    # append these to current batches\n",
    "    texts.extend(chunks)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    \n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Embed each chunk\n",
    "        embed = embed_model.embed_documents([chunk])\n",
    "        embed = np.array(embed).astype('float32')\n",
    "        embeddings.extend(embed)  # Store embeddings in a list\n",
    "        index.add(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "xq = np.array(embed_model.embed_query(\"what is the flamingo model\")).astype('float32')\n",
    "if xq.ndim == 1:\n",
    "    xq = xq.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1108 1617 2502 2726]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes: [[1108 1617 2502 2726]]\n",
      "Results for Query 0:\n",
      "ü¶©\n",
      "Flamingo: a Visual Language Model\n",
      "for Few-Shot Learning\n",
      "Jean-Baptiste Alayrac*,‚Ä°Jeff Donahue*Pauline Luc*Antoine Miech*\n",
      "Iain Barr‚Ä†Yana Hasson‚Ä†Karel Lenc‚Ä†Arthur Mensch‚Ä†Katie Millican‚Ä†\n",
      "Malcolm Reynolds‚Ä†Roman Ring‚Ä†Eliza Rutherford‚Ä†Serkan Cabi Tengda Han\n",
      "Zhitao Gong Sina Samangooei Marianne Monteiro Jacob Menick\n",
      "Sebastian Borgeaud Andrew Brock Aida Nematzadeh Sahand Sharifzadeh\n",
      "Mikolaj Binkowski Ricardo Barreira Oriol Vinyals Andrew Zisserman\n",
      "Karen Simonyan*,‚Ä°\n",
      "*Equal contributions, ordered alphabetically,‚Ä†Equal contributions, ordered alphabetically,\n",
      "‚Ä°Equal senior contributions\n",
      "DeepMind\n",
      "Abstract\n",
      "Building models that can be rapidly adapted to novel tasks using only a handful of\n",
      "annotated examples is an open challenge for multimodal machine learning research.\n",
      "We introduce Flamingo, a family of Visual Language Models (VLM) with this\n",
      "ability. We propose key architectural innovations to: (i) bridge powerful pretrained\n",
      "vision-only and language-only models, (ii) handle sequences of arbitrarily inter-\n",
      "leaved visual and textual data, and (iii) seamlessly ingest images or videos as\n",
      "inputs. Thanks to their Ô¨Çexibility, Flamingo models can be trained on large-scale\n",
      "multimodal web corpora containing arbitrarily interleaved text and images, which\n",
      "is key to endow them with in-context few-shot learning capabilities. We perform a\n",
      "thorough evaluation of our models, exploring and measuring their ability to rapidly\n",
      "adapt to a variety of image and video tasks. These include open-ended tasks such\n",
      "as visual question-answering, where the model is prompted with a question which\n",
      "enhance the domain specific capability of the general domain flamingo model, DeMMo inserts an\n",
      "additional domain specific medical encoder to the perceiver resampler of flamingo. A parameter ef-\n",
      "ficient prompt tuning method is adopted to fine-tune the model for medical domain while preserving\n",
      "its generalization ability.\n",
      "5.1 F LAMINGO AS MODEL BASIS\n",
      "Flamingo is a family of vision language model that is capable of generating language conditioned\n",
      "on interleaved text and image sequences. By connecting visual encoder and LLM with a perceiver\n",
      "resampler, Flamingo models the likelihood of text output yconditioned on interleaved image and\n",
      "text input as a next-token prediction task: pŒ∏(y|x) =QL\n",
      "l=1p(yl|y<l, x<l), where y<landx<lare\n",
      "the sets of text and image tokens preceding yl, thel-th input text token.\n",
      "References\n",
      "[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\n",
      "Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\n",
      "Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy\n",
      "Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n",
      "Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In\n",
      "Advances in Neural Information Processing Systems (NeurIPS) , 2022.\n",
      "[2]Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha Helal, and Aly Fahmy. Automated radiology\n",
      "report generation using conditioned transformers. Informatics in Medicine Unlocked , 24:100557, 2021.\n",
      "[3]Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard\n",
      "Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey,\n",
      "Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin\n",
      "Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez\n",
      "References\n",
      "[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\n",
      "Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\n",
      "Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy\n",
      "Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n",
      "Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In\n",
      "Advances in Neural Information Processing Systems (NeurIPS) , 2022.\n",
      "[2]Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha Helal, and Aly Fahmy. Automated radiology\n",
      "report generation using conditioned transformers. Informatics in Medicine Unlocked , 24:100557, 2021.\n",
      "[3]Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard\n",
      "Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey,\n",
      "Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin\n",
      "Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez\n"
     ]
    }
   ],
   "source": [
    "# Perform the FAISS search\n",
    "D, I = index.search(xq, k)  # I contains the indexes of the closest vectors\n",
    "print(\"Indexes:\", I)\n",
    "\n",
    "# Retrieve the corresponding texts\n",
    "for i, indexes in enumerate(I):\n",
    "    print(f\"Results for Query {i}:\")\n",
    "    for idx in indexes:\n",
    "        if idx >= 0:  # Check if the index is valid\n",
    "            print(texts[idx])\n",
    "        else:\n",
    "            print(\"No valid result for this index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes: [[1108 1617 2502 2726]]\n"
     ]
    }
   ],
   "source": [
    "# Perform the FAISS search\n",
    "D, I = index.search(xq, k)  # I contains the indexes of the closest vectors\n",
    "print(\"Indexes:\", I)\n",
    "\n",
    "context_texts = []\n",
    "for i, indexes in enumerate(I):\n",
    "    for idx in indexes:\n",
    "        context = metadatas[idx]['text']\n",
    "        context_texts.append(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_prompt(query, embed_model, index, k=5):\n",
    "    \"\"\"\n",
    "    Generates a prompt for a Retrieve-and-Generate (RAG) task using a FAISS database for similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The user's question or query as a string.\n",
    "    - embed_model: The embedding model used to convert text to vectors.\n",
    "    - index: The FAISS index containing the embeddings.\n",
    "    - k: The number of similar documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    A string that serves as a prompt for the language model, incorporating context from the similarity search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the query to a vector using the embedding model\n",
    "    query_vector = np.array(embed_model.embed_query(query)).astype('float32')\n",
    "    if query_vector.ndim == 1:\n",
    "        query_vector = xq.reshape(1, -1)\n",
    "    \n",
    "    # Perform the similarity search in the FAISS index\n",
    "    D, I = index.search(query_vector, k)\n",
    "    \n",
    "    # Retrieve and format the text of the relevant document chunks\n",
    "    context_texts = []\n",
    "    for i, indexes in enumerate(I):\n",
    "        for idx in indexes:\n",
    "            context = metadatas[idx]['text']\n",
    "            context_texts.append(context)\n",
    "    \n",
    "    # Construct the prompt by combining the query with the retrieved contexts\n",
    "    prompt = f\"Question: {query}\\n\\n\"\n",
    "    prompt += \"Relevant information:\\n\"\n",
    "    for i, text in enumerate(context_texts, start=1):\n",
    "        prompt += f\"\\n{i}. {text}\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the flamingo model?\n",
      "\n",
      "Relevant information:\n",
      "\n",
      "1. ü¶©\n",
      "Flamingo: a Visual Language Model\n",
      "for Few-Shot Learning\n",
      "Jean-Baptiste Alayrac*,‚Ä°Jeff Donahue*Pauline Luc*Antoine Miech*\n",
      "Iain Barr‚Ä†Yana Hasson‚Ä†Karel Lenc‚Ä†Arthur Mensch‚Ä†Katie Millican‚Ä†\n",
      "Malcolm Reynolds‚Ä†Roman Ring‚Ä†Eliza Rutherford‚Ä†Serkan Cabi Tengda Han\n",
      "Zhitao Gong Sina Samangooei Marianne Monteiro Jacob Menick\n",
      "Sebastian Borgeaud Andrew Brock Aida Nematzadeh Sahand Sharifzadeh\n",
      "Mikolaj Binkowski Ricardo Barreira Oriol Vinyals Andrew Zisserman\n",
      "Karen Simonyan*,‚Ä°\n",
      "*Equal contributions, ordered alphabetically,‚Ä†Equal contributions, ordered alphabetically,\n",
      "‚Ä°Equal senior contributions\n",
      "DeepMind\n",
      "Abstract\n",
      "Building models that can be rapidly adapted to novel tasks using only a handful of\n",
      "annotated examples is an open challenge for multimodal machine learning research.\n",
      "We introduce Flamingo, a family of Visual Language Models (VLM) with this\n",
      "ability. We propose key architectural innovations to: (i) bridge powerful pretrained\n",
      "vision-only and language-only models, (ii) handle sequences of arbitrarily inter-\n",
      "leaved visual and textual data, and (iii) seamlessly ingest images or videos as\n",
      "inputs. Thanks to their Ô¨Çexibility, Flamingo models can be trained on large-scale\n",
      "multimodal web corpora containing arbitrarily interleaved text and images, which\n",
      "is key to endow them with in-context few-shot learning capabilities. We perform a\n",
      "thorough evaluation of our models, exploring and measuring their ability to rapidly\n",
      "adapt to a variety of image and video tasks. These include open-ended tasks such\n",
      "as visual question-answering, where the model is prompted with a question which\n",
      "\n",
      "2. enhance the domain specific capability of the general domain flamingo model, DeMMo inserts an\n",
      "additional domain specific medical encoder to the perceiver resampler of flamingo. A parameter ef-\n",
      "ficient prompt tuning method is adopted to fine-tune the model for medical domain while preserving\n",
      "its generalization ability.\n",
      "5.1 F LAMINGO AS MODEL BASIS\n",
      "Flamingo is a family of vision language model that is capable of generating language conditioned\n",
      "on interleaved text and image sequences. By connecting visual encoder and LLM with a perceiver\n",
      "resampler, Flamingo models the likelihood of text output yconditioned on interleaved image and\n",
      "text input as a next-token prediction task: pŒ∏(y|x) =QL\n",
      "l=1p(yl|y<l, x<l), where y<landx<lare\n",
      "the sets of text and image tokens preceding yl, thel-th input text token.\n",
      "\n",
      "3. References\n",
      "[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\n",
      "Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\n",
      "Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy\n",
      "Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n",
      "Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In\n",
      "Advances in Neural Information Processing Systems (NeurIPS) , 2022.\n",
      "[2]Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha Helal, and Aly Fahmy. Automated radiology\n",
      "report generation using conditioned transformers. Informatics in Medicine Unlocked , 24:100557, 2021.\n",
      "[3]Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard\n",
      "Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey,\n",
      "Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin\n",
      "Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the flamingo model?\"\n",
    "prompt = generate_rag_prompt(query, embed_model, index, k=3)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='String theory is a theoretical framework in physics that attempts to explain the fundamental nature of particles and forces in the universe. It proposes that the most basic building blocks of the universe are not point-like particles, but rather tiny vibrating strings. These strings can vibrate at different frequencies, giving rise to different particles and forces.\\n\\nString theory is a complex and mathematically rigorous theory that attempts to unify quantum mechanics and general relativity, which are currently the two main pillars of modern physics. While string theory has not yet been proven experimentally, it has generated a lot of interest and research in the physics community due to its potential to provide a unified description of all fundamental forces in the universe.\\n\\nIf you have any specific questions about string theory or would like more detailed information, feel free to ask!')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in physics that attempts to explain the fundamental nature of particles and forces in the universe. It proposes that the most basic building blocks of the universe are not point-like particles, but rather tiny vibrating strings. These strings can vibrate at different frequencies, giving rise to different particles and forces.\n",
      "\n",
      "String theory is a complex and mathematically rigorous theory that attempts to unify quantum mechanics and general relativity, which are currently the two main pillars of modern physics. While string theory has not yet been proven experimentally, it has generated a lot of interest and research in the physics community due to its potential to provide a unified description of all fundamental forces in the universe.\n",
      "\n",
      "If you have any specific questions about string theory or would like more detailed information, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flamingo model is a visual language model developed for few-shot learning tasks in multimodal machine learning research. It is capable of rapidly adapting to novel tasks using only a handful of annotated examples. The Flamingo model bridges pretrained vision-only and language-only models, handles sequences of visual and textual data, and can ingest images or videos as inputs. By training on large-scale multimodal corpora, the Flamingo model is endowed with in-context few-shot learning capabilities. It generates language conditioned on interleaved text and image sequences and can be fine-tuned for specific domains, such as the medical domain, while maintaining its generalization ability.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt using FAISS vector db\n",
    "prompt = HumanMessage(\n",
    "    content=generate_rag_prompt(query, embed_model, index, k=3)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User friendly chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model with OpenAI API key and model specification\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "# Start with a system message to define the chatbot's role\n",
    "messages = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "\n",
    "# Function to add a new human message and get the AI's response\n",
    "def add_message_and_get_response(user_input):\n",
    "    human_msg = HumanMessage(content=user_input)\n",
    "    messages.append(human_msg)\n",
    "    \n",
    "    # Optionally, generate a prompt from the user input using your custom function (if needed)\n",
    "    # For a more complex interaction where you need to generate a prompt based on external data or processing\n",
    "    prompt = generate_rag_prompt(user_input, embed_model, index, k=3)\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "\n",
    "    # Get the response from the chat model\n",
    "    res = chat(messages)\n",
    "    \n",
    "    # Add the AI response to the messages history\n",
    "    messages.append(AIMessage(content=res.content))\n",
    "    \n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"AI: {res.content}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! I'm your helpful assistant. How can I help you today? Type 'exit' to end the conversation.\n",
      "User: What is the flamingo model?\n",
      "AI: The Flamingo model is a family of Visual Language Models (VLM) designed for few-shot learning tasks. It incorporates architectural innovations to bridge pretrained vision-only and language-only models, handle sequences of visual and textual data, and seamlessly process images or videos as inputs. Flamingo models are trained on large-scale multimodal corpora containing interleaved text and images, enabling rapid adaptation to novel tasks with minimal annotated examples. The model has been evaluated for various tasks such as visual question-answering and demonstrates the ability to adapt quickly to different image and video tasks.\n",
      "\n",
      "\n",
      "User: what are some use cases for it?\n",
      "AI: Some potential use cases for the Flamingo model include:\n",
      "\n",
      "1. Few-shot Learning: Flamingo can be utilized for tasks where a model needs to adapt quickly to new tasks with minimal annotated examples, making it suitable for scenarios where data is limited.\n",
      "\n",
      "2. Visual Question-Answering: The model can be applied to tasks that involve answering questions based on visual content, demonstrating its ability to understand and generate responses to queries related to images and videos.\n",
      "\n",
      "3. Multimodal Data Processing: Flamingo's capability to handle sequences of interleaved visual and textual data makes it well-suited for processing and generating insights from diverse types of data sources, such as text and images.\n",
      "\n",
      "4. Domain-Specific Applications: The model's flexibility allows for domain-specific adaptations, as seen in the example of enhancing Flamingo with a medical encoder for medical domain tasks, showcasing its potential for specialized applications.\n",
      "\n",
      "Overall, Flamingo's versatility and ability to handle multimodal data make it applicable to various tasks requiring efficient adaptation and understanding of both visual and textual information.\n",
      "\n",
      "\n",
      "User: why is this model a breakthough?\n",
      "AI: The Flamingo model represents a breakthrough in the field of multimodal machine learning research for several reasons:\n",
      "\n",
      "1. **Few-Shot Learning Capabilities:** One of the key challenges in machine learning is the ability to adapt quickly to new tasks with only a few annotated examples. Flamingo addresses this challenge by demonstrating remarkable few-shot learning capabilities, enabling rapid adaptation to novel tasks with minimal labeled data.\n",
      "\n",
      "2. **Architectural Innovations:** Flamingo introduces key architectural innovations that bridge pretrained vision-only and language-only models, handle sequences of interleaved visual and textual data, and seamlessly process images or videos as inputs. These innovations enhance the model's flexibility and adaptability to diverse types of data.\n",
      "\n",
      "3. **Multimodal Processing:** The model's ability to process and generate insights from multimodal data, such as text and images, is a significant advancement. Flamingo can effectively handle sequences of interleaved text and image data, making it suitable for tasks that require understanding and reasoning across different modalities.\n",
      "\n",
      "4. **Domain-Specific Adaptations:** Flamingo's flexibility allows for domain-specific adaptations, as demonstrated by the example of enhancing the model with a medical encoder for specialized medical domain tasks. This capability showcases the model's versatility and potential for application in various domains.\n",
      "\n",
      "5. **Thorough Evaluation:** The thorough evaluation of Flamingo models, including measuring their ability to rapidly adapt to a variety of image and video tasks, provides empirical evidence of the model's effectiveness and performance in real-world scenarios.\n",
      "\n",
      "In summary, the Flamingo model's combination of few-shot learning capabilities, architectural innovations, multimodal processing abilities, domain-specific adaptability, and rigorous evaluation make it a breakthrough in multimodal machine learning research.\n",
      "\n",
      "\n",
      "User: what is the perceiver resampler?\n",
      "AI: The perceiver resampler is a component within the Flamingo model that plays a crucial role in generating language conditioned on interleaved text and image sequences. Specifically, the perceiver resampler connects the visual encoder and the Language-to-Language Model (LLM) in the Flamingo architecture.\n",
      "\n",
      "The perceiver resampler is responsible for modeling the likelihood of text output conditioned on interleaved image and text input. It operates as a next-token prediction task within the Flamingo model, contributing to the model's ability to generate meaningful language responses based on the input sequences of images and text.\n",
      "\n",
      "In summary, the perceiver resampler in the Flamingo model helps facilitate the integration of visual and textual information, enabling the model to process and generate language outputs based on sequences of both image and text data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Welcome message to the user\n",
    "print(\"Chatbot: Hi! I'm your helpful assistant. How can I help you today? Type 'exit' to end the conversation.\")\n",
    "\n",
    "# Main loop to keep the chat going\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    add_message_and_get_response(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
